{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import operator\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_file = sys.argv[1]\n",
    "# output_file = sys.argv[2]\n",
    "\n",
    "input_file = \"../data/yelp_train.csv\"\n",
    "output_file = \"../result/task1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/usr/local/Cellar/apache-spark/2.4.5/libexec/./bin/spark-submit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/data-mining/lib/python3.10/site-packages/pyspark/context.py:144\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/envs/data-mining/lib/python3.10/site-packages/pyspark/context.py?line=138'>139</a>\u001b[0m \u001b[39mif\u001b[39;00m gateway \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m gateway\u001b[39m.\u001b[39mgateway_parameters\u001b[39m.\u001b[39mauth_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/envs/data-mining/lib/python3.10/site-packages/pyspark/context.py?line=139'>140</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/envs/data-mining/lib/python3.10/site-packages/pyspark/context.py?line=140'>141</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/envs/data-mining/lib/python3.10/site-packages/pyspark/context.py?line=141'>142</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is not allowed as it is a security risk.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='file:///Users/rhythmgirdhar/miniforge3/envs/data-mining/lib/python3.10/site-packages/pyspark/context.py?line=143'>144</a>\u001b[0m SparkContext\u001b[39m.\u001b[39;49m_ensure_initialized(\u001b[39mself\u001b[39;49m, gateway\u001b[39m=\u001b[39;49mgateway, conf\u001b[39m=\u001b[39;49mconf)\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/envs/data-mining/lib/python3.10/site-packages/pyspark/context.py?line=144'>145</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/envs/data-mining/lib/python3.10/site-packages/pyspark/context.py?line=145'>146</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/envs/data-mining/lib/python3.10/site-packages/pyspark/context.py?line=146'>147</a>\u001b[0m                   conf, jsc, profiler_cls)\n",
      "File \u001b[0;32m~/miniforge3/envs/data-mining/lib/python3.10/site-packages/pyspark/context.py:339\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/envs/data-mining/lib/python3.10/site-packages/pyspark/context.py?line=336'>337</a>\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/envs/data-mining/lib/python3.10/site-packages/pyspark/context.py?line=337'>338</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_gateway:\n\u001b[0;32m--> <a href='file:///Users/rhythmgirdhar/miniforge3/envs/data-mining/lib/python3.10/site-packages/pyspark/context.py?line=338'>339</a>\u001b[0m         SparkContext\u001b[39m.\u001b[39m_gateway \u001b[39m=\u001b[39m gateway \u001b[39mor\u001b[39;00m launch_gateway(conf)\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/envs/data-mining/lib/python3.10/site-packages/pyspark/context.py?line=339'>340</a>\u001b[0m         SparkContext\u001b[39m.\u001b[39m_jvm \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_gateway\u001b[39m.\u001b[39mjvm\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/envs/data-mining/lib/python3.10/site-packages/pyspark/context.py?line=341'>342</a>\u001b[0m     \u001b[39mif\u001b[39;00m instance:\n",
      "File \u001b[0;32m~/miniforge3/envs/data-mining/lib/python3.10/site-packages/pyspark/java_gateway.py:98\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/rhythmgirdhar/miniforge3/envs/data-mining/lib/python3.10/site-packages/pyspark/java_gateway.py?line=95'>96</a>\u001b[0m         signal\u001b[39m.\u001b[39msignal(signal\u001b[39m.\u001b[39mSIGINT, signal\u001b[39m.\u001b[39mSIG_IGN)\n\u001b[1;32m     <a href='file:///Users/rhythmgirdhar/miniforge3/envs/data-mining/lib/python3.10/site-packages/pyspark/java_gateway.py?line=96'>97</a>\u001b[0m     popen_kwargs[\u001b[39m'\u001b[39m\u001b[39mpreexec_fn\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m preexec_func\n\u001b[0;32m---> <a href='file:///Users/rhythmgirdhar/miniforge3/envs/data-mining/lib/python3.10/site-packages/pyspark/java_gateway.py?line=97'>98</a>\u001b[0m     proc \u001b[39m=\u001b[39m Popen(command, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpopen_kwargs)\n\u001b[1;32m     <a href='file:///Users/rhythmgirdhar/miniforge3/envs/data-mining/lib/python3.10/site-packages/pyspark/java_gateway.py?line=98'>99</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/envs/data-mining/lib/python3.10/site-packages/pyspark/java_gateway.py?line=99'>100</a>\u001b[0m     \u001b[39m# preexec_fn not supported on Windows\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/envs/data-mining/lib/python3.10/site-packages/pyspark/java_gateway.py?line=100'>101</a>\u001b[0m     proc \u001b[39m=\u001b[39m Popen(command, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpopen_kwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/data-mining/lib/python3.10/subprocess.py:971\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/envs/data-mining/lib/python3.10/subprocess.py?line=966'>967</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_mode:\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/envs/data-mining/lib/python3.10/subprocess.py?line=967'>968</a>\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mTextIOWrapper(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr,\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/envs/data-mining/lib/python3.10/subprocess.py?line=968'>969</a>\u001b[0m                     encoding\u001b[39m=\u001b[39mencoding, errors\u001b[39m=\u001b[39merrors)\n\u001b[0;32m--> <a href='file:///Users/rhythmgirdhar/miniforge3/envs/data-mining/lib/python3.10/subprocess.py?line=970'>971</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_child(args, executable, preexec_fn, close_fds,\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/envs/data-mining/lib/python3.10/subprocess.py?line=971'>972</a>\u001b[0m                         pass_fds, cwd, env,\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/envs/data-mining/lib/python3.10/subprocess.py?line=972'>973</a>\u001b[0m                         startupinfo, creationflags, shell,\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/envs/data-mining/lib/python3.10/subprocess.py?line=973'>974</a>\u001b[0m                         p2cread, p2cwrite,\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/envs/data-mining/lib/python3.10/subprocess.py?line=974'>975</a>\u001b[0m                         c2pread, c2pwrite,\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/envs/data-mining/lib/python3.10/subprocess.py?line=975'>976</a>\u001b[0m                         errread, errwrite,\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/envs/data-mining/lib/python3.10/subprocess.py?line=976'>977</a>\u001b[0m                         restore_signals,\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/envs/data-mining/lib/python3.10/subprocess.py?line=977'>978</a>\u001b[0m                         gid, gids, uid, umask,\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/envs/data-mining/lib/python3.10/subprocess.py?line=978'>979</a>\u001b[0m                         start_new_session)\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/envs/data-mining/lib/python3.10/subprocess.py?line=979'>980</a>\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/envs/data-mining/lib/python3.10/subprocess.py?line=980'>981</a>\u001b[0m     \u001b[39m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/envs/data-mining/lib/python3.10/subprocess.py?line=981'>982</a>\u001b[0m     \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m \u001b[39mfilter\u001b[39m(\u001b[39mNone\u001b[39;00m, (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstdin, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstdout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr)):\n",
      "File \u001b[0;32m~/miniforge3/envs/data-mining/lib/python3.10/subprocess.py:1847\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/rhythmgirdhar/miniforge3/envs/data-mining/lib/python3.10/subprocess.py?line=1844'>1845</a>\u001b[0m     \u001b[39mif\u001b[39;00m errno_num \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   <a href='file:///Users/rhythmgirdhar/miniforge3/envs/data-mining/lib/python3.10/subprocess.py?line=1845'>1846</a>\u001b[0m         err_msg \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> <a href='file:///Users/rhythmgirdhar/miniforge3/envs/data-mining/lib/python3.10/subprocess.py?line=1846'>1847</a>\u001b[0m     \u001b[39mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   <a href='file:///Users/rhythmgirdhar/miniforge3/envs/data-mining/lib/python3.10/subprocess.py?line=1847'>1848</a>\u001b[0m \u001b[39mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/usr/local/Cellar/apache-spark/2.4.5/libexec/./bin/spark-submit'"
     ]
    }
   ],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_RDD = sc.textFile(input_file)\n",
    "header = data_RDD.first()\n",
    "data_RDD = data_RDD.filter(lambda row: row != header).map(lambda row: row.split(\",\")).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_user = data_RDD.map(lambda row: (row[1], row[0])).groupByKey().mapValues(set)\n",
    "\n",
    "business_user_dict = {}\n",
    "for business, users in business_user.collect():\n",
    "    business_user_dict[business] = users\n",
    "\n",
    "\n",
    "user_index_dict = data_RDD.map(lambda kv: kv[0]).distinct() \\\n",
    "        .sortBy(lambda item: item).zipWithIndex().map(lambda kv: {kv[0]: kv[1]}) \\\n",
    "        .flatMap(lambda kv_items: kv_items.items()).collectAsMap()\n",
    "\n",
    "index_user_dict = {v: k for k, v in user_index_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 60\n",
    "m = len(index_user_dict) * 2\n",
    "\n",
    "func_list = list()\n",
    "param_as = random.sample(range(1, m), n)\n",
    "func_list.append(param_as)\n",
    "param_bs = random.sample(range(1, m), n)\n",
    "func_list.append(param_bs)\n",
    "\n",
    "print(func_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 233333333333\n",
    "sign_dict = dict()\n",
    "for business, users in business_user_dict.items():\n",
    "    minhash_sign_list = list()\n",
    "    for i in range(n):\n",
    "        minhash = float(\"inf\")\n",
    "        for user in users:\n",
    "            minhash = min(minhash, (((func_list[0][i] * user_index_dict[user] + func_list[1][i]) % p) % m))\n",
    "        minhash_sign_list.append(int(minhash))\n",
    "    sign_dict[business] = minhash_sign_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 2\n",
    "b = n // r\n",
    "\n",
    "bands_dict = dict()\n",
    "for business, minhash_sign in sign_dict.items():\n",
    "    for i in range(b):\n",
    "        index = (i, tuple(minhash_sign[i * r: i * r + r]))\n",
    "        if index not in bands_dict.keys():\n",
    "            bands_dict[index] = []\n",
    "        bands_dict[index].append(business)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_dict = {key: values for key, values in bands_dict.items() if len(values) > 1}\n",
    "\n",
    "candidate_pairs = set()\n",
    "for values in candidate_dict.values():\n",
    "    sorted_values = sorted(values)\n",
    "    comb_list = combinations(sorted_values, 2)\n",
    "    for item in comb_list:\n",
    "        candidate_pairs.add(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_header = \"business_id_1, business_id_2, similarity\\n\"\n",
    "\n",
    "result_str = \"\"\n",
    "\n",
    "for bus1, bus2 in candidate_pairs:\n",
    "    user1 = business_user_dict[bus1]\n",
    "    user2 = business_user_dict[bus2]\n",
    "    jaccard = len(user1 & user2) / len(user1 | user2)\n",
    "\n",
    "    if jaccard >= 0.5:\n",
    "        result_str += str(bus1) + \",\" + str(bus2) + \",\" + str(jaccard) + \"\\n\"\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.writelines(result_header)\n",
    "    f.writelines(result_str)\n",
    "    \n",
    "\"\"\"\n",
    "Calculate precision and recall\n",
    "\"\"\"\n",
    "with open(\"../data/pure_jaccard_similarity.csv\") as in_file:\n",
    "    answer = in_file.read().splitlines(True)[1:]\n",
    "answer_set = set()\n",
    "for line in answer:\n",
    "    row = line.split(',')\n",
    "    answer_set.add((row[0], row[1]))\n",
    "with open(\"../result/task1.csv\") as in_file:\n",
    "    estimate = in_file.read().splitlines(True)[1:]\n",
    "estimate_set = set()\n",
    "for line in estimate:\n",
    "    row = line.split(',')\n",
    "    estimate_set.add((row[0], row[1]))\n",
    "print(\"Precision:\")\n",
    "print(len(answer_set.intersection(estimate_set))/len(estimate_set))\n",
    "print(\"Recall:\")\n",
    "print(len(answer_set.intersection(estimate_set))/len(answer_set))\n",
    "print(answer_set.difference(estimate_set))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train_file = \"../data/yelp_train.csv\"\n",
    "input_test_file = \"../data/yelp_val.csv\"\n",
    "output_file = \"../result/task2.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_RDD = sc.textFile(input_train_file)\n",
    "header = data_RDD.first()\n",
    "train_data_RDD = train_data_RDD.filter(lambda row: row != header).map(lambda row: row.split(\",\"))\n",
    "\n",
    "\n",
    "test_data_RDD = sc.textFile(input_test_file)\n",
    "header = data_RDD.first()\n",
    "test_data_RDD = test_data_RDD.filter(lambda row: row != header).map(lambda row: row.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_index_dict = train_data_RDD.map(lambda kvv: kvv[0]).distinct() \\\n",
    "        .sortBy(lambda item: item).zipWithIndex().map(lambda kv: {kv[0]: kv[1]}) \\\n",
    "        .flatMap(lambda kv_items: kv_items.items()).collectAsMap()\n",
    "\n",
    "reversed_index_user_dict = {v: k for k, v in user_index_dict.items()}\n",
    "\n",
    "business_index_dict = train_data_RDD.map(lambda kvv: kvv[1]).distinct() \\\n",
    "        .sortBy(lambda item: item).zipWithIndex().map(lambda kv: {kv[0]: kv[1]}) \\\n",
    "        .flatMap(lambda kv_items: kv_items.items()).collectAsMap()\n",
    "\n",
    "reversed_index_business_dict = {v: k for k, v in business_index_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data_RDD' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m business_user_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_data_RDD\u001b[49m\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m row: (row[\u001b[38;5;241m1\u001b[39m], row[\u001b[38;5;241m0\u001b[39m]))\u001b[38;5;241m.\u001b[39mgroupByKey()\u001b[38;5;241m.\u001b[39mmapValues(\u001b[38;5;28mset\u001b[39m)\u001b[38;5;241m.\u001b[39mcollectAsMap()\n\u001b[1;32m      2\u001b[0m user_bus_train \u001b[38;5;241m=\u001b[39m train_data_RDD\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m row: (row[\u001b[38;5;241m1\u001b[39m], row[\u001b[38;5;241m0\u001b[39m]))\u001b[38;5;241m.\u001b[39mgroupByKey()\u001b[38;5;241m.\u001b[39mmapValues(\u001b[38;5;28mset\u001b[39m)\u001b[38;5;241m.\u001b[39mcollectAsMap()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data_RDD' is not defined"
     ]
    }
   ],
   "source": [
    "business_user_dict = train_data_RDD.map(lambda row: (row[1], row[0])).groupByKey().mapValues(set).collectAsMap()\n",
    "user_bus_train = train_data_RDD.map(lambda row: (row[1], row[0])).groupByKey().mapValues(set).collectAsMap()\n",
    "# bus_avg = train_data_RDD.map(lambda row: (row[0], float(row[2]))).groupByKey().mapValues(list).map(lambda x: (x[0], sum(x[1]) / len(x[1]))).collectAsMap()\n",
    "# user_avg = train_data_RDD.map(lambda row: (row[1], float(row[2]))).groupByKey().mapValues(list).map(lambda x: (x[0], sum(x[1]) / len(x[1]))).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f68d252a7c1ef1728efea60b176a0ee91efa29040333f8f87d6876a32e13cd12"
  },
  "kernelspec": {
   "display_name": "Python 3.10.9 ('data-mining')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
