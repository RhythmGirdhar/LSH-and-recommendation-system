{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import operator\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_file = sys.argv[1]\n",
    "# output_file = sys.argv[2]\n",
    "\n",
    "input_file = \"../data/yelp_train.csv\"\n",
    "output_file = \"../result/task1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at /var/folders/34/pk85xwtn36j4bnlsbz8kt3vh0000gn/T/ipykernel_26268/1542122174.py:1 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/pyspark/context.py:195\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/lib/python3.10/site-packages/pyspark/context.py?line=188'>189</a>\u001b[0m \u001b[39mif\u001b[39;00m gateway \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m gateway\u001b[39m.\u001b[39mgateway_parameters\u001b[39m.\u001b[39mauth_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/lib/python3.10/site-packages/pyspark/context.py?line=189'>190</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/lib/python3.10/site-packages/pyspark/context.py?line=190'>191</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/lib/python3.10/site-packages/pyspark/context.py?line=191'>192</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is not allowed as it is a security risk.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/lib/python3.10/site-packages/pyspark/context.py?line=192'>193</a>\u001b[0m     )\n\u001b[0;32m--> <a href='file:///Users/rhythmgirdhar/miniforge3/lib/python3.10/site-packages/pyspark/context.py?line=194'>195</a>\u001b[0m SparkContext\u001b[39m.\u001b[39;49m_ensure_initialized(\u001b[39mself\u001b[39;49m, gateway\u001b[39m=\u001b[39;49mgateway, conf\u001b[39m=\u001b[39;49mconf)\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/lib/python3.10/site-packages/pyspark/context.py?line=195'>196</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/lib/python3.10/site-packages/pyspark/context.py?line=196'>197</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_init(\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/lib/python3.10/site-packages/pyspark/context.py?line=197'>198</a>\u001b[0m         master,\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/lib/python3.10/site-packages/pyspark/context.py?line=198'>199</a>\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/lib/python3.10/site-packages/pyspark/context.py?line=207'>208</a>\u001b[0m         udf_profiler_cls,\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/lib/python3.10/site-packages/pyspark/context.py?line=208'>209</a>\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/pyspark/context.py:430\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/lib/python3.10/site-packages/pyspark/context.py?line=426'>427</a>\u001b[0m     callsite \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_active_spark_context\u001b[39m.\u001b[39m_callsite\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/lib/python3.10/site-packages/pyspark/context.py?line=428'>429</a>\u001b[0m     \u001b[39m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/rhythmgirdhar/miniforge3/lib/python3.10/site-packages/pyspark/context.py?line=429'>430</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/lib/python3.10/site-packages/pyspark/context.py?line=430'>431</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot run multiple SparkContexts at once; \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/lib/python3.10/site-packages/pyspark/context.py?line=431'>432</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mexisting SparkContext(app=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, master=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/lib/python3.10/site-packages/pyspark/context.py?line=432'>433</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m created by \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m at \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/lib/python3.10/site-packages/pyspark/context.py?line=433'>434</a>\u001b[0m         \u001b[39m%\u001b[39m (\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/lib/python3.10/site-packages/pyspark/context.py?line=434'>435</a>\u001b[0m             currentAppName,\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/lib/python3.10/site-packages/pyspark/context.py?line=435'>436</a>\u001b[0m             currentMaster,\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/lib/python3.10/site-packages/pyspark/context.py?line=436'>437</a>\u001b[0m             callsite\u001b[39m.\u001b[39mfunction,\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/lib/python3.10/site-packages/pyspark/context.py?line=437'>438</a>\u001b[0m             callsite\u001b[39m.\u001b[39mfile,\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/lib/python3.10/site-packages/pyspark/context.py?line=438'>439</a>\u001b[0m             callsite\u001b[39m.\u001b[39mlinenum,\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/lib/python3.10/site-packages/pyspark/context.py?line=439'>440</a>\u001b[0m         )\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/lib/python3.10/site-packages/pyspark/context.py?line=440'>441</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/lib/python3.10/site-packages/pyspark/context.py?line=441'>442</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/rhythmgirdhar/miniforge3/lib/python3.10/site-packages/pyspark/context.py?line=442'>443</a>\u001b[0m     SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39m=\u001b[39m instance\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at /var/folders/34/pk85xwtn36j4bnlsbz8kt3vh0000gn/T/ipykernel_26268/1542122174.py:1 "
     ]
    }
   ],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_RDD = sc.textFile(input_file)\n",
    "header = data_RDD.first()\n",
    "data_RDD = data_RDD.filter(lambda row: row != header).map(lambda row: row.split(\",\")).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_user = data_RDD.map(lambda row: (row[1], row[0])).groupByKey().mapValues(set)\n",
    "\n",
    "business_user_dict = {}\n",
    "for business, users in business_user.collect():\n",
    "    business_user_dict[business] = users\n",
    "\n",
    "\n",
    "user_index_dict = data_RDD.map(lambda kv: kv[0]).distinct() \\\n",
    "        .sortBy(lambda item: item).zipWithIndex().map(lambda kv: {kv[0]: kv[1]}) \\\n",
    "        .flatMap(lambda kv_items: kv_items.items()).collectAsMap()\n",
    "\n",
    "index_user_dict = {v: k for k, v in user_index_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 60\n",
    "m = len(index_user_dict) * 2\n",
    "\n",
    "func_list = list()\n",
    "param_as = random.sample(range(1, m), n)\n",
    "func_list.append(param_as)\n",
    "param_bs = random.sample(range(1, m), n)\n",
    "func_list.append(param_bs)\n",
    "\n",
    "print(func_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 233333333333\n",
    "sign_dict = dict()\n",
    "for business, users in business_user_dict.items():\n",
    "    minhash_sign_list = list()\n",
    "    for i in range(n):\n",
    "        minhash = float(\"inf\")\n",
    "        for user in users:\n",
    "            minhash = min(minhash, (((func_list[0][i] * user_index_dict[user] + func_list[1][i]) % p) % m))\n",
    "        minhash_sign_list.append(int(minhash))\n",
    "    sign_dict[business] = minhash_sign_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 2\n",
    "b = n // r\n",
    "\n",
    "bands_dict = dict()\n",
    "for business, minhash_sign in sign_dict.items():\n",
    "    for i in range(b):\n",
    "        index = (i, tuple(minhash_sign[i * r: i * r + r]))\n",
    "        if index not in bands_dict.keys():\n",
    "            bands_dict[index] = []\n",
    "        bands_dict[index].append(business)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_dict = {key: values for key, values in bands_dict.items() if len(values) > 1}\n",
    "\n",
    "candidate_pairs = set()\n",
    "for values in candidate_dict.values():\n",
    "    sorted_values = sorted(values)\n",
    "    comb_list = combinations(sorted_values, 2)\n",
    "    for item in comb_list:\n",
    "        candidate_pairs.add(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:\n",
      "1.0\n",
      "Recall:\n",
      "1.0\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "result_header = \"business_id_1, business_id_2, similarity\\n\"\n",
    "\n",
    "result_str = \"\"\n",
    "\n",
    "for bus1, bus2 in candidate_pairs:\n",
    "    user1 = business_user_dict[bus1]\n",
    "    user2 = business_user_dict[bus2]\n",
    "    jaccard = len(user1 & user2) / len(user1 | user2)\n",
    "\n",
    "    if jaccard >= 0.5:\n",
    "        result_str += str(bus1) + \",\" + str(bus2) + \",\" + str(jaccard) + \"\\n\"\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.writelines(result_header)\n",
    "    f.writelines(result_str)\n",
    "    \n",
    "\"\"\"\n",
    "Calculate precision and recall\n",
    "\"\"\"\n",
    "with open(\"../data/pure_jaccard_similarity.csv\") as in_file:\n",
    "    answer = in_file.read().splitlines(True)[1:]\n",
    "answer_set = set()\n",
    "for line in answer:\n",
    "    row = line.split(',')\n",
    "    answer_set.add((row[0], row[1]))\n",
    "with open(\"../result/task1.csv\") as in_file:\n",
    "    estimate = in_file.read().splitlines(True)[1:]\n",
    "estimate_set = set()\n",
    "for line in estimate:\n",
    "    row = line.split(',')\n",
    "    estimate_set.add((row[0], row[1]))\n",
    "print(\"Precision:\")\n",
    "print(len(answer_set.intersection(estimate_set))/len(estimate_set))\n",
    "print(\"Recall:\")\n",
    "print(len(answer_set.intersection(estimate_set))/len(answer_set))\n",
    "print(answer_set.difference(estimate_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f68d252a7c1ef1728efea60b176a0ee91efa29040333f8f87d6876a32e13cd12"
  },
  "kernelspec": {
   "display_name": "Python 3.10.9 ('data-mining')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
